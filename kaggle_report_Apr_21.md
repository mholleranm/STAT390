# Kaggle competition

## Team: Matt Holleran Meyer

## Methodology

To start, we did some feature selection using a basic random forest and support vector machine from the sklearn libraries. We extracted any feature that
had an importance over -.5 and were left with 667 features. After this, I tried out an MLP, RF, and XGBoost. The RF and xgboost performed the best so 
we will be moving forward with fine tuning those with grid search for now. 

Next week, we look forward to implementing cross-validation, finding optimal parameters for RF and XGboost, and experimenting with better mlp models.

We also hope to begin feature engineering using LDA, PCA, and autoencoders

## Current Progress

Ranked two on the leaderboard
